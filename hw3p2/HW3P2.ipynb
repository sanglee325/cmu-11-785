{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykh8vQc0p71n"
      },
      "source": [
        "# 11785 HW3P2: Automatic Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0m4w_KkMeQ"
      },
      "source": [
        "Welcome to HW3P2. In this homework, you will be using the same data from HW1 but will be incorporating sequence models. We recommend you get familaried with sequential data and the working of RNNs, LSTMs and GRUs to have a smooth learning in this part of the homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEll1kGkhcR"
      },
      "source": [
        "Disclaimer: This starter notebook will not be as elaborate as that of HW1P2 or HW2P2. You will need to do most of the implementation in this notebook because, it is expected after 2 HWs, you will be in a position to write a notebook from scratch. You are welcomed to reuse the code from the previous starter notebooks but may also need to make appropriate changes for this homework. <br>\n",
        "We have also given you 3 log files for the Very Low Cutoff (Levenshtein Distance = 30) so that you can observe how loss decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHGaJ_8tx_5Z"
      },
      "source": [
        "Common errors which you may face\n",
        "\n",
        "\n",
        "*   Shape errors: Half of the errors from this homework will account to this category. Try printing the shapes between intermediate steps to debug\n",
        "*   CUDA out of Memory: When your architecture has a lot of parameters, this can happen. Golden keys for this is, (1) Reducing batch_size (2) Call *torch.cuda.empty_cache* often, even inside your training loop, (3) Call *gc.collect* if it helps and (4) Restart run time if nothing works\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "c8f8d454-1e76-492e-dcf9-3e2d16d38b35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/psl/.conda/envs/hw2p2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "from data.phonemes import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUCKqm1ST1sU"
      },
      "source": [
        "# Dataset and dataloading (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "# This cell is where your actual TODOs start\n",
        "# You will need to implement the Dataset class by your own. You may also implement it similar to HW1P2 (dont require context)\n",
        "# The steps for implementation given below are how we have implemented it.\n",
        "# However, you are welcomed to do it your own way if it is more comfortable or efficient. \n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
        "\n",
        "        self.X_dir = os.path.join(data_path, partition, \"mfcc\")\n",
        "        self.Y_dir = os.path.join(data_path, partition, \"transcript\")\n",
        "        \n",
        "        self.X_files = os.listdir(self.X_dir)\n",
        "        self.Y_files = os.listdir(self.Y_dir)\n",
        "\n",
        "        # TODO: store PHONEMES from phonemes.py inside the class. phonemes.py will be downloaded from kaggle.\n",
        "        # You may wish to store PHONEMES as a class attribute or a global variable as well.\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        assert(len(self.X_files) == len(self.Y_files))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "    \n",
        "        X = np.load(os.path.join(self.X_dir, self.X_files[ind])) # TODO: Load the mfcc npy file at the ind in the directory\n",
        "        X = (X - X.mean(axis=0)) / X.std(axis=0) # ADD: normalize\n",
        "        X = torch.FloatTensor(X)\n",
        "       \n",
        "        Y = np.load(os.path.join(self.Y_dir, self.Y_files[ind])) # TODO: Load the corresponding transcripts\n",
        "        labels = np.asarray([self.PHONEMES.index(yy) for yy in Y[1:-1]]) # TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
        "        \n",
        "        # Remember, the transcripts are a sequence of phonemes. Eg. np.array(['<sos>', 'B', 'IH', 'K', 'SH', 'AA', '<eos>'])\n",
        "        # You need to convert these into a sequence of Long tensors\n",
        "        # Tip: You may need to use self.PHONEMES\n",
        "        # Remember, PHONEMES or PHONEME_MAP do not have '<sos>' or '<eos>' but the transcripts have them. \n",
        "        # You need to remove '<sos>' and '<eos>' from the trancripts. \n",
        "        # Inefficient way is to use a for loop for this. Efficient way is to think that '<sos>' occurs at the start and '<eos>' occurs at the end.\n",
        "        \n",
        "        Yy = torch.LongTensor(labels) # TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
        "\n",
        "        return X, Yy\n",
        "    \n",
        "    def collate_fn(batch):\n",
        "        batch_x = [x for x,y in batch]\n",
        "        batch_y = [y for x,y in batch]\n",
        "\n",
        "        batch_x_pad = pad_sequence(batch_x, batch_first=True)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [sample.shape[0] for sample in batch_x] # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y, batch_first=True) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [sample.shape[0] for sample in batch_y] # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "\n",
        "# You can either try to combine test data in the previous class or write a new Dataset class for test data\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, test_order): # test_order is the csv similar to what you used in hw1\n",
        "\n",
        "        test_order_list = os.path.join(data_path, \"test\", test_order)\n",
        "        self.X_dir = os.path.join(data_path, \"test\", \"mfcc\")\n",
        "        self.X_files = []\n",
        "\n",
        "        with open(test_order_list) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                self.X_files.append(row[0])\n",
        "            self.X_files = self.X_files[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "    \n",
        "    def __getitem__(self, ind):\n",
        "        # TODOs: Need to return only X because this is the test dataset\n",
        "    \n",
        "        X = np.load(os.path.join(self.X_dir, self.X_files[ind]))\n",
        "        \n",
        "        return torch.from_numpy(X)\n",
        "    \n",
        "    def collate_fn(batch):\n",
        "        batch_x = [x for x in batch]\n",
        "        batch_x_pad = pad_sequence(batch_x, batch_first=True) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [x.shape[0] for x in batch_x] # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 28539, batches = 223\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 21\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "root = 'data' # TODO: Where your hw3p2_student_data folder is\n",
        "\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root, 'dev')\n",
        "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
        "                            num_workers=4, collate_fn=LibriSamples.collate_fn) # TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, \n",
        "                            num_workers=4, collate_fn=LibriSamples.collate_fn) # TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, \n",
        "                            num_workers=4, collate_fn=LibriSamplesTest.collate_fn) # TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u9FwVZ9I2da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 2267, 13]) torch.Size([128, 280]) torch.Size([128]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# Optional\n",
        "# Test code for checking shapes and return arguments of the train and val loaders\n",
        "for data in val_loader:\n",
        "    x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Configuration (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (lstm): LSTM(13, 256, batch_first=True)\n",
            "  (classification): Linear(in_features=256, out_features=41, bias=True)\n",
            ")\n",
            "torch.Size([128, 2267, 13]) tensor([ 980,  553,  636,  417,  642,  642, 1399, 1107,  400, 1023,  270,  257,\n",
            "         381,  399,  427, 2267,  861,  469, 2054, 1532,  555,  855,  857, 1410,\n",
            "         482,  482,  259,  276,  609,  452,  358,  596,  969,  781,  952,  294,\n",
            "         767,  380,  276, 2081,  396,  278,  195,  287, 1006,  484,  248,  292,\n",
            "        1480,  731,  279, 1177,  446,  729,  279, 1187,  214,  234, 1138,  353,\n",
            "         442,  308,  630,  290,  681,  333,  824,  249,  405,  336,  304,  801,\n",
            "         528, 1101,  219, 1419,  480,  793,  647,  734,  444,  501,  515,  217,\n",
            "        2100,  245,  765,  413,  434,  953,  661,  498,  679, 1746,  542,  889,\n",
            "         384,  202,  304,  465,  416, 1268,  441,  281,  796,  576,  294, 1561,\n",
            "         445,  225, 1334, 1254, 1909, 1196, 1942, 1028,  570,  994, 1446,  264,\n",
            "         658,  821,  428, 1521,  425, 1548,  797,  914])\n",
            "=================================================================\n",
            "                 Kernel Shape     Output Shape  Params  Mult-Adds\n",
            "Layer                                                            \n",
            "0_lstm                      -     [91673, 256]  277504     275456\n",
            "1_classification    [256, 41]  [2267, 128, 41]   10537      10496\n",
            "-----------------------------------------------------------------\n",
            "                      Totals\n",
            "Total params          288041\n",
            "Trainable params      288041\n",
            "Non-trainable params       0\n",
            "Mult-Adds             285952\n",
            "=================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[91673, 256]</td>\n",
              "      <td>277504</td>\n",
              "      <td>275456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_classification</th>\n",
              "      <td>[256, 41]</td>\n",
              "      <td>[2267, 128, 41]</td>\n",
              "      <td>10537</td>\n",
              "      <td>10496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Kernel Shape     Output Shape  Params  Mult-Adds\n",
              "Layer                                                            \n",
              "0_lstm                      -     [91673, 256]  277504     275456\n",
              "1_classification    [256, 41]  [2267, 128, 41]   10537      10496"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size=13, hidden_size=256, num_layers=1, num_classes=41): # You can add any extra arguments as you wish\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
        "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
        "        # self.embedding = \n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)# TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
        "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "        self.classification = nn.Linear(hidden_size, num_classes)# TODO: Create a single classification layer using nn.Linear()\n",
        "\n",
        "    def forward(self, x, X_lens): # TODO: You need to pass atleast 1 more parameter apart from self and x\n",
        "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
        "        packed_input = pack_padded_sequence(x, X_lens, batch_first=True, enforce_sorted=False)# TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
        "\n",
        "        out1, (out2, out3) = self.lstm(packed_input) # TODO: Pass packed input to self.lstm\n",
        "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
        "        out, lengths = pad_packed_sequence(out1) # TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
        "\n",
        "        out = self.classification(out) # TODO: Pass unpacked LSTM output to the classification layer\n",
        "        out = F.log_softmax(out, dim=2) # Optional: Do log softmax on the output. Which dimension?\n",
        "\n",
        "        return out, lengths # TODO: Need to return 2 variables\n",
        "\n",
        "model = Network().to(device)\n",
        "print(model)\n",
        "print(x.shape, lx)\n",
        "summary(model, x.to(device), lx) # x and lx are from the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Configuration (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CTCLoss() # TODO: What loss do you need for sequence to sequence models? \n",
        "# Do you need to transpose or permute the model output to find out the loss? Read its documentation\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3) # TODO: Adam works well with LSTM (use lr = 2e-3)\n",
        "decoder = CTCBeamDecoder(\n",
        "                            PHONEMES,\n",
        "                            model_path=None,\n",
        "                            alpha=0,\n",
        "                            beta=0,\n",
        "                            cutoff_top_n=40,\n",
        "                            cutoff_prob=1.0,\n",
        "                            beam_width=100,\n",
        "                            num_processes=4,\n",
        "                            blank_id=0,\n",
        "                            log_probs_input=False\n",
        "                        ) # TODO: Intialize the CTC beam decoder\n",
        "# Check out https://github.com/parlance/ctcdecode for the details on how to implement decoding\n",
        "# Do you need to give log_probs_input = True or False?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KEuvs3Kje47-"
      },
      "outputs": [],
      "source": [
        "# this function calculates the Levenshtein distance \n",
        "\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, PHONEME_MAP):\n",
        "\n",
        "    # h - ouput from the model. Probability distributions at each time step \n",
        "    # y - target output sequence - sequence of Long tensors\n",
        "    # lh, ly - Lengths of output and \n",
        "    # decoder - decoder object which was initialized in the previous cell\n",
        "    # PHONEME_MAP - maps output to a character to find the  distance\n",
        "\n",
        "    # TODO: You may need to transpose or permute h based on how you passed it to the criterion\n",
        "    # Print out the shapes often to debug\n",
        "\n",
        "    # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs\n",
        "    # Input to the decode method will be h and its lengths lh \n",
        "    # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
        "    beam_result, beam_scores, timesteps, out_len = decoder.decode(h, seq_lens=lh)\n",
        "\n",
        "    batch_size = y.shape[0]# TODO\n",
        "\n",
        "    dist = 0\n",
        "\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "        h_sliced = beam_result[i,:out_len] # TODO: Get the output as a sequence of numbers from beam_results\n",
        "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
        "        # Same goes for beam_results and out_lens\n",
        "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
        "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
        "\n",
        "        h_string = [PHONEME_MAP.index(hh) for hh in h_sliced] # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "\n",
        "        y_sliced = y[i,:ly] # TODO: Do the same for y - slice off the padding with ly\n",
        "        y_string = [PHONEME_MAP.index(yy) for yy in y_sliced] # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        \n",
        "        dist += Levenshtein.distance(h_string, y_string)\n",
        "\n",
        "    dist/=batch_size\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d7b7iY0we8Kj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.2402\n",
            "| epoch   1 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0570\n",
            "| epoch   1 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.09 | loss/phoneme 0.0421\n",
            "| epoch   1 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.11 | loss/phoneme 0.0370\n",
            "| epoch   1 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.14 | loss/phoneme 0.0344\n",
            "| epoch   1 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.17 | loss/phoneme 0.0329\n"
          ]
        }
      ],
      "source": [
        "def train(epoch, model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    sum_loss, total = 0, 0\n",
        "    total_dist = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    log_interval = 40\n",
        "\n",
        "    for batch, (data) in enumerate(train_loader):\n",
        "        x, y, lx, ly = data\n",
        "        total += len(y)\n",
        "\n",
        "        x = x.to(device)\n",
        "        outputs, length = model(x, lx)\n",
        "\n",
        "        loss = criterion(outputs, y, length, ly)\n",
        "        loss.backward()\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "\n",
        "        #total_dist += calculate_levenshtein(outputs, y, length, ly, decoder, PHONEME_MAP)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % log_interval == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            avg_loss = sum_loss / total\n",
        "            print(\n",
        "                '| epoch {:3d} | {:5d}/{:5d} batches ({:5.2f}%) | lr {:.2e} | {:3.0f} ms/utter | loss/utter {:5.2f} | loss/phoneme {:5.4f}'\n",
        "                .format(epoch+1, batch, len(train_loader), (100.0 * batch / len(train_loader)),\n",
        "                        optimizer.param_groups[0]['lr'],\n",
        "                        elapsed * 1000.0 / (log_interval * batch_size),\n",
        "                        sum_loss / (log_interval * batch_size),\n",
        "                        avg_loss))\n",
        "            start_time = time.time()\n",
        "\n",
        "train(0, model, train_loader, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0278\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache() # Use this often\n",
        "\n",
        "# TODO: Write the model evaluation function if you want to validate after every epoch\n",
        "\n",
        "# You are free to write your own code for model evaluation or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Note that when you give a higher beam width, decoding will take a longer time to get executed\n",
        "# Therefore, it is recommended that you calculate only the val dataset's Levenshtein distance (train not recommended) with a small beam width\n",
        "# When you are evaluating on your test set, you may have a higher beam width\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    sum_loss, total_num = 0, 0\n",
        "    total_dist = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_interval = 40\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (data) in enumerate(val_loader):\n",
        "            x, y, lx, ly = data\n",
        "            total_num += len(y)\n",
        "\n",
        "            x = x.to(device)\n",
        "            outputs, length = model(x, lx)\n",
        "\n",
        "            loss = criterion(outputs, y, length, ly)\n",
        "            sum_loss += loss.item()\n",
        "\n",
        "\n",
        "            #total_dist += calculate_levenshtein(outputs, y, length, ly, decoder, PHONEME_MAP)\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    avg_loss = sum_loss / total_num\n",
        "    print(\n",
        "        '| Validation | {:5d}/{:5d} batches ({:5.2f}%) | lr {:.2e} | {:3.0f} ms/utter | loss/utter {:5.2f} | loss/phoneme {:5.4f}'\n",
        "        .format(batch, len(val_loader), (100.0 * batch / len(val_loader)),\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                elapsed * 1000.0 / (log_interval * batch_size),\n",
        "                sum_loss / (log_interval * batch_size),\n",
        "                avg_loss))\n",
        "                    \n",
        "\n",
        "validate(model, val_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0265\n",
            "| epoch   1 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.03 | loss/phoneme 0.0265\n",
            "| epoch   1 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.05 | loss/phoneme 0.0263\n",
            "| epoch   1 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.08 | loss/phoneme 0.0262\n",
            "| epoch   1 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.10 | loss/phoneme 0.0258\n",
            "| epoch   1 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.13 | loss/phoneme 0.0254\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0232\n",
            "| epoch   2 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0224\n",
            "| epoch   2 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.02 | loss/phoneme 0.0221\n",
            "| epoch   2 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.04 | loss/phoneme 0.0217\n",
            "| epoch   2 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0213\n",
            "| epoch   2 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.08 | loss/phoneme 0.0209\n",
            "| epoch   2 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.10 | loss/phoneme 0.0205\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0193\n",
            "| epoch   3 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0188\n",
            "| epoch   3 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.02 | loss/phoneme 0.0182\n",
            "| epoch   3 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.04 | loss/phoneme 0.0179\n",
            "| epoch   3 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.05 | loss/phoneme 0.0176\n",
            "| epoch   3 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.07 | loss/phoneme 0.0173\n",
            "| epoch   3 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.09 | loss/phoneme 0.0171\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0162\n",
            "| epoch   4 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0155\n",
            "| epoch   4 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.02 | loss/phoneme 0.0154\n",
            "| epoch   4 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.03 | loss/phoneme 0.0153\n",
            "| epoch   4 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.05 | loss/phoneme 0.0151\n",
            "| epoch   4 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0150\n",
            "| epoch   4 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.07 | loss/phoneme 0.0148\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0148\n",
            "| epoch   5 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0140\n",
            "| epoch   5 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.01 | loss/phoneme 0.0140\n",
            "| epoch   5 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.03 | loss/phoneme 0.0139\n",
            "| epoch   5 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.04 | loss/phoneme 0.0138\n",
            "| epoch   5 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0137\n",
            "| epoch   5 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.07 | loss/phoneme 0.0136\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0138\n",
            "| epoch   6 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0132\n",
            "| epoch   6 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.01 | loss/phoneme 0.0130\n",
            "| epoch   6 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.03 | loss/phoneme 0.0129\n",
            "| epoch   6 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.04 | loss/phoneme 0.0129\n",
            "| epoch   6 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.05 | loss/phoneme 0.0128\n",
            "| epoch   6 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0127\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0130\n",
            "| epoch   7 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0123\n",
            "| epoch   7 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.01 | loss/phoneme 0.0123\n",
            "| epoch   7 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.02 | loss/phoneme 0.0122\n",
            "| epoch   7 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.04 | loss/phoneme 0.0121\n",
            "| epoch   7 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.05 | loss/phoneme 0.0121\n",
            "| epoch   7 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0120\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0123\n",
            "| epoch   8 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0116\n",
            "| epoch   8 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.01 | loss/phoneme 0.0115\n",
            "| epoch   8 |    80/  223 batches (35.87%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.02 | loss/phoneme 0.0114\n",
            "| epoch   8 |   120/  223 batches (53.81%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.03 | loss/phoneme 0.0114\n",
            "| epoch   8 |   160/  223 batches (71.75%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.05 | loss/phoneme 0.0114\n",
            "| epoch   8 |   200/  223 batches (89.69%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.06 | loss/phoneme 0.0113\n",
            "| Validation |    21/   22 batches (95.45%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.01 | loss/phoneme 0.0117\n",
            "| epoch   9 |     0/  223 batches ( 0.00%) | lr 2.00e-03 |   0 ms/utter | loss/utter  0.00 | loss/phoneme 0.0108\n",
            "| epoch   9 |    40/  223 batches (17.94%) | lr 2.00e-03 |   2 ms/utter | loss/utter  0.01 | loss/phoneme 0.0109\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# TODO: Write the model training code \n",
        "\n",
        "# You are free to write your own code for training or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Tip: Implement mixed precision training\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    train(epoch, model, train_loader, optimizer, criterion)\n",
        "    validate(model, val_loader, criterion)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROrqXnNqzJSc"
      },
      "source": [
        "# Submit to kaggle (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "def decode(output, seq_sizes, beam_width=60):\n",
        "    decoder = CTCBeamDecoder(labels=PHONEME_MAP, blank_id=0, beam_width=beam_width)\n",
        "    output = torch.transpose(output, 0, 1) \n",
        "    probs = F.softmax(output, dim=2).data\n",
        "\n",
        "    output, scores, timesteps, out_seq_len = decoder.decode(probs=probs, seq_lens=seq_sizes)\n",
        "\n",
        "\n",
        "    decoded = []\n",
        "    for i in range(output.size(0)):\n",
        "        chrs = \"\"\n",
        "        if out_seq_len[i, 0] != 0:\n",
        "            chrs = \"\".join(PHONEME_MAP[o] for o in output[i, 0, :out_seq_len[i, 0]])\n",
        "        decoded.append(chrs)\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# TODO: Write your model evaluation code for the test dataset\n",
        "# You can write your own code or use from the previous homewoks' stater notebooks\n",
        "# You can't calculate loss here. Why?\n",
        "csv_path = './submission.csv'\n",
        "with open(csv_path, 'w') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=['id', 'predictions'])\n",
        "        writer.writeheader()\n",
        "        cnt = 0\n",
        "        with torch.no_grad():\n",
        "              for batch, (x, lx) in enumerate(test_loader):\n",
        "                    x = x.to(device)\n",
        "                    output, length = model(x, lx)\n",
        "\n",
        "                    decoded = decode(output, length, beam_width=100)\n",
        "                    for s in decoded:\n",
        "                        writer.writerow({\"id\": cnt, \"predictions\": s})\n",
        "                        cnt += 1\n",
        "print(\"done\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "HW3P2_Starter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
